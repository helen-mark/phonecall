import ollama
import chromadb
from sentence_transformers import SentenceTransformer
import re
from typing import List, Dict, Optional
import numpy as np


class EnhancedUniversalAnalyzer(UniversalCallAnalyzer):
    def ask_with_evidence(self, question: str) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–≤–µ—Ç —Å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞–º–∏"""

        relevant_calls = self._find_relevant_calls(question)

        if not relevant_calls:
            return {
                "answer": "–í –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å.",
                "evidence": [],
                "confidence": 0
            }

        analysis = self._analyze_with_evidence(question, relevant_calls)

        return {
            "answer": analysis,
            "evidence": relevant_calls[:3],  # –¢–æ–ø-3 –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞
            "confidence": self._calculate_confidence(question, relevant_calls),
            "sources_count": len(relevant_calls)
        }

    def _analyze_with_evidence(self, question: str, relevant_calls: List[str]) -> str:
        """–ê–Ω–∞–ª–∏–∑ —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤"""

        context = "\n".join([
            f"–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ {i + 1}: {call}"
            for i, call in enumerate(relevant_calls[:5])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
        ])

        prompt = f"""
–í–û–ü–†–û–°: {question}

–ù–ê–ô–î–ï–ù–ù–´–ï –î–ê–ù–ù–´–ï:
{context}

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∞–Ω–Ω—ã–µ –∏ –æ—Ç–≤–µ—Ç—å:
1. –ï—Å—Ç—å –ª–∏ –ø—Ä—è–º–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –æ—Ç–≤–µ—Ç–∞?
2. –ï—Å–ª–∏ –µ—Å—Ç—å - –ø—Ä–∏–≤–µ–¥–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã
3. –ï—Å–ª–∏ –Ω–µ—Ç - —É–∫–∞–∂–∏, —á—Ç–æ –∏–º–µ–Ω–Ω–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
4. –ë—É–¥—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–µ–Ω

–ê–ù–ê–õ–ò–ó –ò –û–¢–í–ï–¢:
"""
        response = self.client.generate(
            model=self.model_name,
            prompt=prompt,
            options={'temperature': 0.1}
        )

        return response['response']

    def _calculate_confidence(self, question: str, relevant_calls: List[str]) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –æ—Ç–≤–µ—Ç–µ"""
        if not relevant_calls:
            return 0.0

        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: —á–µ–º –±–æ–ª—å—à–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —Ç–µ–º –≤—ã—à–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        max_results = 10
        confidence = min(len(relevant_calls) / max_results, 1.0)

        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–∏ —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è—Ö
        question_words = set(re.findall(r'\b\w+\b', question.lower()))
        for call in relevant_calls[:3]:
            call_words = set(re.findall(r'\b\w+\b', call.lower()))
            if question_words.intersection(call_words):
                confidence += 0.2

        return min(confidence, 1.0)


# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏
def test_enhanced_analyzer():
    analyzer = EnhancedUniversalAnalyzer()

    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    call_texts = [...]  # –≤–∞—à–∏ –∑–≤–æ–Ω–∫–∏

    analyzer.index_calls(call_texts)

    hard_questions = [
        "–ù–∞–∑—ã–≤–∞–ª –ª–∏ –º–µ–Ω–µ–¥–∂–µ—Ä –∫–ª–∏–µ–Ω—Ç–∞ –¥—É—Ä–∞–∫–æ–º –≤—á–µ—Ä–∞?",
        "–ö—Ç–æ –∏–∑ –∫–ª–∏–µ–Ω—Ç–æ–≤ –∂–∞–ª–æ–≤–∞–ª—Å—è –∏–º–µ–Ω–Ω–æ –Ω–∞ –ü–µ—Ç—Ä–æ–≤–∞?",
        "–ë—ã–ª–∏ –ª–∏ —É–≥—Ä–æ–∑—ã –≤ –∞–¥—Ä–µ—Å –∫–ª–∏–µ–Ω—Ç–æ–≤?",
    ]

    for question in hard_questions:
        print(f"\nüîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º: {question}")
        result = analyzer.ask_with_evidence(question)

        print(f"‚úÖ –û—Ç–≤–µ—Ç: {result['answer']}")
        print(f"üìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.2f}")
        print(f"üìé –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {result['sources_count']}")
        if result['evidence']:
            print("üîé –î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞:")
            for i, evidence in enumerate(result['evidence'][:2], 1):
                print(f"   {i}. {evidence[:100]}...")

class UniversalCallAnalyzer:
    def __init__(self, model_name: str = "llama3:8b"):
        self.model_name = model_name
        self.client = ollama.Client()

        # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.embedding_model = SentenceTransformer(
            'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
        )

        # –í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.chroma_client = chromadb.Client()
        self.collection = self.chroma_client.create_collection(
            name="calls_universal",
            metadata={"description": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –±–∞–∑–∞ –∑–≤–æ–Ω–∫–æ–≤ –¥–ª—è –ª—é–±—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"}
        )

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: –∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.keyword_index = {}

    def index_calls(self, call_texts: List[str]):
        """–ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –∑–≤–æ–Ω–∫–∏ –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        print(f"üìö –ò–Ω–¥–µ–∫—Å–∏—Ä—É—é {len(call_texts)} –∑–≤–æ–Ω–∫–æ–≤...")

        for i, text in enumerate(call_texts):
            # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ
            embedding = self.embedding_model.encode(text).tolist()

            self.collection.add(
                documents=[text],
                embeddings=[embedding],
                ids=[f"call_{i}"],
                metadatas=[{"call_id": i, "length": len(text)}]
            )

            # –¢–æ—á–Ω–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ (–¥–ª—è –∏–º–µ–Ω, –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ñ—Ä–∞–∑)
            self._build_keyword_index(text, i)

        print("‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –≥–æ—Ç–æ–≤–∞ –¥–ª—è –ª—é–±—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤!")

    def _build_keyword_index(self, text: str, call_id: int):
        """–°—Ç—Ä–æ–∏—Ç –∏–Ω–¥–µ–∫—Å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        words = re.findall(r'\b\w+\b', text.lower())
        for word in words:
            if len(word) > 3:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                if word not in self.keyword_index:
                    self.keyword_index[word] = []
                self.keyword_index[word].append(call_id)

    def ask_anything(self, question: str, max_results: int = 10) -> str:
        """–ó–∞–¥–∞–µ—Ç –ª—é–±–æ–π –≤–æ–ø—Ä–æ—Å –ø–æ –±–∞–∑–µ –∑–≤–æ–Ω–∫–æ–≤"""
        print(f"üîç –ò—â—É –æ—Ç–≤–µ—Ç –Ω–∞: '{question}'")

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
        semantic_results = self._semantic_search(question, max_results)

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        keyword_results = self._keyword_search(question)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        all_relevant_calls = self._merge_results(semantic_results, keyword_results)

        if not all_relevant_calls:
            return "‚ùå –í –±–∞–∑–µ –∑–≤–æ–Ω–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å."

        print(f"üìû –ù–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(all_relevant_calls)}")

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω–æ–µ
        return self._analyze_with_context(question, all_relevant_calls)

    def _semantic_search(self, question: str, max_results: int) -> List[str]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ —Å–º—ã—Å–ª—É"""
        question_embedding = self.embedding_model.encode(question).tolist()

        results = self.collection.query(
            query_embeddings=[question_embedding],
            n_results=max_results
        )

        return results['documents'][0] if results['documents'] else []

    def _keyword_search(self, question: str) -> List[str]:
        """–¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        relevant_call_ids = set()
        words = re.findall(r'\b\w+\b', question.lower())

        for word in words:
            if word in self.keyword_index:
                relevant_call_ids.update(self.keyword_index[word])

        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç—ã –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∑–≤–æ–Ω–∫–æ–≤
        keyword_results = []
        for call_id in list(relevant_call_ids)[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
            results = self.collection.get(ids=[f"call_{call_id}"])
            if results['documents']:
                keyword_results.extend(results['documents'])
                return keyword_results

    def _merge_results(self, semantic_results: List[str], keyword_results: List[str]) -> List[str]:
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –ø–æ–∏—Å–∫–∞"""
        all_results = semantic_results + keyword_results
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (–ø—Ä–æ—Å—Ç–µ–π—à–∏–º —Å–ø–æ—Å–æ–±–æ–º)
        unique_results = []
        seen_texts = set()

        for result in all_results:
            text_hash = hash(result[:100])  # –•–µ—à–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
            if text_hash not in seen_texts:
                seen_texts.add(text_hash)
                unique_results.append(result)

        return unique_results[:15]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ

    def _analyze_with_context(self, question: str, relevant_calls: List[str]) -> str:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å"""

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –º–æ–¥–µ–ª–∏
        context = "\n\n".join([
            f"[–§—Ä–∞–≥–º–µ–Ω—Ç {i + 1}]: {call}"
            for i, call in enumerate(relevant_calls)
        ])

        prompt = f"""
    –¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∞–∑—É —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤. 
    –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤. 
    –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç - –≥–æ–≤–æ—Ä–∏ "–í –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± —ç—Ç–æ–º".

    –í–û–ü–†–û–°: {question}

    –ë–ê–ó–ê –î–ê–ù–ù–´–• –†–ê–ó–ì–û–í–û–†–û–í:
    {context}

    –ò–ù–°–¢–†–£–ö–¶–ò–ò:
    1. –û—Ç–≤–µ—á–∞–π —Ç–æ—á–Ω–æ –Ω–∞ –∑–∞–¥–∞–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å
    2. –ï—Å–ª–∏ –Ω—É–∂–Ω–æ - —Ü–∏—Ç–∏—Ä—É–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
    3. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ - —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏
    4. –ë—É–¥—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫–æ–Ω–∫—Ä–µ—Ç–µ–Ω

    –û–¢–í–ï–¢:
    """
        try:
            response = self.client.generate(
                model=self.model_name,
                prompt=prompt,
                options={
                    'temperature': 0.1,  # –ú–∏–Ω–∏–º—É–º –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
                    'num_predict': 1000
                }
            )
            return response['response']
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {str(e)}"

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏
def demo_universal_questions():
    analyzer = UniversalCallAnalyzer()

    # –ü—Ä–∏–º–µ—Ä –±–∞–∑—ã –∑–≤–æ–Ω–∫–æ–≤ (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ - —Ç—ã—Å—è—á–∏)
    call_texts = [
        "–ú–µ–Ω–µ–¥–∂–µ—Ä: –ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, —á–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å? –ö–ª–∏–µ–Ω—Ç: –£ –º–µ–Ω—è –ø—Ä–æ–±–ª–µ–º–∞ —Å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–æ–º. –ú–µ–Ω–µ–¥–∂–µ—Ä: –°–µ–π—á–∞—Å –ø–æ—Å–º–æ—Ç—Ä–∏–º... –í—ã –ø—Ä–∞–≤—ã, –µ—Å—Ç—å —Å–±–æ–∏.",
        "–ö–ª–∏–µ–Ω—Ç: –í–∞—à —Å–æ—Ç—Ä—É–¥–Ω–∏–∫ –Ω–∞–∑–≤–∞–ª –º–µ–Ω—è –≥–ª—É–ø—ã–º! –ú–µ–Ω–µ–¥–∂–µ—Ä: –ò–∑–≤–∏–Ω–∏—Ç–µ, —Ç–∞–∫–æ–≥–æ –±—ã—Ç—å –Ω–µ –¥–æ–ª–∂–Ω–æ. –ö–∞–∫ –∑–≤–∞–ª–∏ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞? –ö–ª–∏–µ–Ω—Ç: –ù–µ –ø–æ–º–Ω—é, –Ω–æ —ç—Ç–æ –±—ã–ª–æ –≤—á–µ—Ä–∞.",
        "–ú–µ–Ω–µ–¥–∂–µ—Ä –ü–µ—Ç—Ä–æ–≤: –ê–ª–ª–æ? –ö–ª–∏–µ–Ω—Ç: –ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ, —è —Ö–æ—á—É –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å—Å—è. –ú–µ–Ω–µ–¥–∂–µ—Ä –ü–µ—Ç—Ä–æ–≤: –°–ª—É—à–∞—é –≤–∞—Å. –ö–ª–∏–µ–Ω—Ç: –ú–µ–Ω—è —Ç–æ–ª—å–∫–æ —á—Ç–æ –Ω–∞–∑–≤–∞–ª–∏ –¥—É—Ä–∞–∫–æ–º –≤–∞—à–∏–º –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–º!",
        "–ö–ª–∏–µ–Ω—Ç: –ú–Ω–µ –Ω—É–∂–Ω–∞ –ø–æ–º–æ—â—å —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π —Ä–æ—É—Ç–µ—Ä–∞. –ú–µ–Ω–µ–¥–∂–µ—Ä –°–∏–¥–æ—Ä–æ–≤: –ö–æ–Ω–µ—á–Ω–æ, –ø–æ–º–æ–≥—É. –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫–∞–±–µ–ª—è. –ö–ª–∏–µ–Ω—Ç: –£–∂–µ –ø—Ä–æ–≤–µ—Ä—è–ª.",
        "–ú–µ–Ω–µ–¥–∂–µ—Ä: –î–æ–±—Ä—ã–π –¥–µ–Ω—å! –ö–ª–∏–µ–Ω—Ç: –Ø –≤ —è—Ä–æ—Å—Ç–∏! –í–∞—à —Å–æ—Ç—Ä—É–¥–Ω–∏–∫ –ò–≤–∞–Ω–æ–≤ –Ω–∞—Ö–∞–º–∏–ª –º–Ω–µ! –ú–µ–Ω–µ–¥–∂–µ—Ä: –ü—Ä–∏–Ω–æ—Å–∏–º –∏–∑–≤–∏–Ω–µ–Ω–∏—è, —Ä–∞–∑–±–µ—Ä–µ–º—Å—è.",
        "–ö–ª–∏–µ–Ω—Ç: –ü–æ–¥—Å–∫–∞–∂–∏—Ç–µ —Ç–∞—Ä–∏—Ñ—ã. –ú–µ–Ω–µ–¥–∂–µ—Ä: –ï—Å—Ç—å –ø–∞–∫–µ—Ç –∑–∞ 500 —Ä—É–±–ª–µ–π. –ö–ª–∏–µ–Ω—Ç: –°–ø–∞—Å–∏–±–æ, –ø–æ–¥—É–º–∞—é.",
        "–ú–µ–Ω–µ–¥–∂–µ—Ä: –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å? –ö–ª–∏–µ–Ω—Ç: –£ –º–µ–Ω—è –º–µ–¥–ª–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä–Ω–µ—Ç. –ú–µ–Ω–µ–¥–∂–µ—Ä: –í–æ–∑–º–æ–∂–Ω–æ, –ø—Ä–æ–±–ª–µ–º—ã –Ω–∞ –ª–∏–Ω–∏–∏.",
    ]

    # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –∑–≤–æ–Ω–∫–∏
    analyzer.index_calls(call_texts)

    print("üöÄ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –õ–Æ–ë–´–ú –≤–æ–ø—Ä–æ—Å–∞–º!")
    print("=" * 60)

    # –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
    test_questions = [
        "–ù–∞–∑—ã–≤–∞–ª –ª–∏ –º–µ–Ω–µ–¥–∂–µ—Ä –∫–ª–∏–µ–Ω—Ç–∞ –¥—É—Ä–∞–∫–æ–º –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Å—è—Ü?",
        "–ö—Ç–æ –∏–∑ –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤ —É–ø–æ–º–∏–Ω–∞–ª—Å—è –≤ –∂–∞–ª–æ–±–∞—Ö?",
        "–ë—ã–ª–∏ –ª–∏ —Å–ª—É—á–∞–∏ —Ö–∞–º—Å—Ç–≤–∞ –æ—Ç —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤?",
        "–ß—Ç–æ –∫–ª–∏–µ–Ω—Ç—ã –≥–æ–≤–æ—Ä—è—Ç –ø—Ä–æ —Ç–∞—Ä–∏—Ñ—ã?",
        "–£–ø–æ–º–∏–Ω–∞–ª—Å—è –ª–∏ –º–µ–Ω–µ–¥–∂–µ—Ä –ü–µ—Ç—Ä–æ–≤ –≤ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ?",
        "–°–∫–æ–ª—å–∫–æ —Ä–∞–∑ –∫–ª–∏–µ–Ω—Ç—ã –∂–∞–ª–æ–≤–∞–ª–∏—Å—å –Ω–∞ –º–µ–¥–ª–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä–Ω–µ—Ç?",
        "–ö—Ç–æ —Ç–∞–∫–æ–π –º–µ–Ω–µ–¥–∂–µ—Ä –ò–≤–∞–Ω–æ–≤ –∏ —á—Ç–æ –æ –Ω–µ–º –≥–æ–≤–æ—Ä—è—Ç?",
        "–ö–∞–∫–∏–µ –∏–º–µ–Ω–Ω–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —É–ø–æ–º–∏–Ω–∞–ª–∏—Å—å?",
    ]

    for question in test_questions:
        print(f"\nü§î –í–û–ü–†–û–°: {question}")
        print("‚îÄ" * 50)

        answer = analyzer.ask_anything(question)
        print(f"üìù –û–¢–í–ï–¢: {answer}")

        print("‚îÄ" * 50)

if __name__ == "__main__":
    demo_universal_questions()