import json
import os
import re
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict
from enum import Enum
from collections import defaultdict, Counter
import sqlite3
from contextlib import contextmanager
import pandas as pd
from typing import Union
import ollama
#from llama_cpp import Llama


# ==================== –°—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö ====================

class MetricType(Enum):
    """–¢–∏–ø—ã –º–µ—Ç—Ä–∏–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
    COUNT_BY_TAG = "count_by_tag"
    TOP_N_TAGS = "top_n_tags"
    TAG_TRENDS = "tag_trends"
    COMPARISON = "comparison"
    SUMMARY_STATS = "summary_stats"


@dataclass
class AnalysisPlan:
    """–ü–ª–∞–Ω –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç LLM"""
    time_period: Dict[str, Any]  # start, end, description
    target_tags: List[str]
    metrics: List[MetricType]
    grouping: str = "month"
    comparison_tags: List[str] = None
    additional_filters: Dict = None

    def to_dict(self):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è JSON"""
        return {
            'time_period': self.time_period,
            'target_tags': self.target_tags,
            'metrics': [m.value for m in self.metrics],
            'grouping': self.grouping,
            'comparison_tags': self.comparison_tags,
            'filters': self.additional_filters or {}
        }


# ==================== Google Drive Data Loader ====================

class DriveDataLoader:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ —É–ø—Ä–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ Google Drive JSON —Ñ–∞–π–ª–æ–≤"""

    def __init__(self, json_directory: str, drive_path: str = None):
        self.csv_dir = json_directory
        self.drive_path = drive_path
        self.calls_cache = None
        self.conn = None
        self._check_drive_access()

    def _check_drive_access(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø –∫ Google Drive"""
        if self.drive_path and 'drive' in self.csv_dir:
            print(f"üåê –ò—Å–ø–æ–ª—å–∑—É—é Google Drive: {self.csv_dir}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            if not os.path.exists(self.csv_dir):
                print(f"‚ö†Ô∏è  –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ Drive: {self.csv_dir}")
                print("‚ÑπÔ∏è  –°–æ–∑–¥–∞—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é...")
                os.makedirs(self.csv_dir, exist_ok=True)

                # –°–æ–∑–¥–∞–µ–º README —Ñ–∞–π–ª
                readme_path = os.path.join(self.csv_dir, "README.txt")
                with open(readme_path, 'w', encoding='utf-8') as f:
                    f.write("–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è JSON —Ñ–∞–π–ª–æ–≤ —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã—Ö –∑–≤–æ–Ω–∫–æ–≤\n")
                    f.write("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Å—é–¥–∞ —Ñ–∞–π–ª—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON\n")
                    f.write(f"–°–æ–∑–¥–∞–Ω–æ: {datetime.now()}")

                print(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –≤ Google Drive")
            else:
                print(f"‚úÖ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–∞–π–¥–µ–Ω–∞ –≤ Google Drive")

    def load_all_calls(self, limit: int = None) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –∑–≤–æ–Ω–∫–∏ –∏–∑ CSV —Ñ–∞–π–ª–∞"""
        if self.calls_cache is not None:
            return self.calls_cache[:limit] if limit else self.calls_cache

        all_calls = []

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        if not os.path.exists(self.csv_dir):
            print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.csv_dir}")
            if self.drive_path:
                print(f"‚ÑπÔ∏è  –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ø–∞–ø–∫–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ Google Drive")
                print(f"üìç –û–∂–∏–¥–∞–µ–º—ã–π –ø—É—Ç—å: {self.csv_dir}")
            return []

        # –ò—â–µ–º CSV —Ñ–∞–π–ª—ã
        try:
            csv_files = [f for f in os.listdir(self.csv_dir) if f.endswith('.csv')]
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {e}")
            return []

        if not csv_files:
            print(f"‚ö†Ô∏è  –í –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {self.csv_dir} –Ω–µ—Ç CSV —Ñ–∞–π–ª–æ–≤")
            print("‚ÑπÔ∏è  –û–∂–∏–¥–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç CSV: –∫–æ–ª–æ–Ω–∫–∏ 'date', 'text', 'tags'")
            return []

        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π CSV —Ñ–∞–π–ª (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö)
        csv_file = csv_files[0]
        filepath = os.path.join(self.csv_dir, csv_file)

        print(f"üìÇ –ß–∏—Ç–∞—é –¥–∞–Ω–Ω—ã–µ –∏–∑ CSV —Ñ–∞–π–ª–∞: {csv_file}")

        try:
            # –ß–∏—Ç–∞–µ–º CSV —Ñ–∞–π–ª
            df = pd.read_csv(
                filepath,
                encoding='utf-8',
                parse_dates=['date'],  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–∞—Ä—Å–∏–º –¥–∞—Ç—É
                converters={
                    'tags': lambda x: eval(x) if isinstance(x, str) else []  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É –≤ —Å–ø–∏—Å–æ–∫
                }
            )

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            required_columns = ['date', 'text', 'tags']
            missing_columns = [col for col in required_columns if col not in df.columns]

            if missing_columns:
                print(f"‚ùå –í CSV —Ñ–∞–π–ª–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
                print(f"   –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
                return []

            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ –∏–∑ CSV")

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º DataFrame –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
            for idx, row in df.iterrows():
                # –ü–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É (—É–∂–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ datetime –±–ª–∞–≥–æ–¥–∞—Ä—è parse_dates)
                call_date = pd.to_datetime(row['date'])

                # –ï—Å–ª–∏ tags - —Å—Ç—Ä–æ–∫–∞, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ø–∏—Å–æ–∫
                tags = row['tags']
                if isinstance(tags, str):
                    try:
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–æ—Ä–º–∞—Ç: "['tag1', 'tag2']"
                        tags = eval(tags) if tags.startswith('[') else tags.split(',')
                    except:
                        tags = []

                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∑–∞–ø–∏—Å—å
                call_record = {
                    'id': f"call_{idx}",
                    'file_name': csv_file,
                    'call_date': call_date,
                    'year': call_date.year if pd.notna(call_date) else None,
                    'month': call_date.month if pd.notna(call_date) else None,
                    'day': call_date.day if pd.notna(call_date) else None,
                    'full_text': str(row['text']) if pd.notna(row['text']) else '',
                    'summary': row.get('summary', '') if 'summary' in df.columns else '',
                    'tags': tags if isinstance(tags, list) else [tags],
                    'text_length': len(str(row['text'])) if pd.notna(row['text']) else 0,
                    'source_file': filepath,
                    'drive_path': self.drive_path if self.drive_path else None
                }

                all_calls.append(call_record)

                # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∑–∞–ø–∏—Å–µ–π
                if limit and idx + 1 >= limit:
                    break

            self.calls_cache = all_calls

            # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            print(f"‚úÖ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–æ {len(all_calls)} –∑–∞–ø–∏—Å–µ–π –∑–≤–æ–Ω–∫–æ–≤")

            if self.drive_path:
                print(f"üåê –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ Google Drive")

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö
            if all_calls:
                dates = [c['call_date'] for c in all_calls if c['call_date']]
                if dates:
                    min_date = min(dates)
                    max_date = max(dates)
                    print(f"üìÖ –î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç: {min_date.strftime('%d.%m.%Y')} - {max_date.strftime('%d.%m.%Y')}")

                # –ü–æ–¥—Å—á–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–≥–æ–≤
                all_tags = []
                for call in all_calls:
                    all_tags.extend(call['tags'])
                unique_tags = set(all_tags)
                print(f"üè∑Ô∏è  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–≥–æ–≤: {len(unique_tags)}")

            return all_calls

        except pd.errors.EmptyDataError:
            print(f"‚ùå CSV —Ñ–∞–π–ª {csv_file} –ø—É—Å—Ç–æ–π")
            return []
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è CSV —Ñ–∞–π–ª–∞ {csv_file}: {e}")
            import traceback
            traceback.print_exc()
            return []

    def _extract_date_from_filename(self, filename: str) -> datetime:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—É –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–∞—Ç—ã –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        patterns = [
            r'(\d{4})-(\d{2})-(\d{2})',  # YYYY-MM-DD
            r'(\d{2})\.(\d{2})\.(\d{4})',  # DD.MM.YYYY
            r'(\d{4})(\d{2})(\d{2})',  # YYYYMMDD
        ]

        for pattern in patterns:
            match = re.search(pattern, filename)
            if match:
                groups = match.groups()
                if len(groups) == 3:
                    if pattern == patterns[0]:  # YYYY-MM-DD
                        year, month, day = map(int, groups)
                        return datetime(year, month, day)
                    elif pattern == patterns[1]:  # DD.MM.YYYY
                        day, month, year = map(int, groups)
                        return datetime(year, month, day)
                    elif pattern == patterns[2]:  # YYYYMMDD
                        year, month, day = int(groups[0]), int(groups[1]), int(groups[2])
                        return datetime(year, month, day)

        # –ï—Å–ª–∏ –¥–∞—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞—Ç—É –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞
        filepath = os.path.join(self.csv_dir, filename)
        if os.path.exists(filepath):
            try:
                return datetime.fromtimestamp(os.path.getmtime(filepath))
            except:
                pass

        # Fallback: —Ç–µ–∫—É—â–∞—è –¥–∞—Ç–∞
        return datetime.now()

    def setup_in_memory_db(self):
        """–°–æ–∑–¥–∞–µ—Ç in-memory SQLite –±–∞–∑—É –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        if self.conn is not None:
            return self.conn

        self.conn = sqlite3.connect(':memory:')
        cursor = self.conn.cursor()

        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã
        cursor.execute("""
        CREATE TABLE calls (
            id TEXT PRIMARY KEY,
            file_name TEXT,
            call_date TEXT,
            year INTEGER,
            month INTEGER,
            day INTEGER,
            full_text TEXT,
            summary TEXT,
            tags_json TEXT,
            text_length INTEGER,
            source_file TEXT,
            drive_path TEXT
        )
        """)

        cursor.execute("""
        CREATE TABLE call_tags (
            call_id TEXT,
            tag TEXT,
            FOREIGN KEY (call_id) REFERENCES calls(id)
        )
        """)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        calls = self.load_all_calls()
        for call in calls:
            cursor.execute("""
            INSERT INTO calls (id, file_name, call_date, year, month, day, 
                              full_text, summary, tags_json, text_length, source_file, drive_path)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                call['id'],
                call['file_name'],
                call['call_date'].isoformat(),
                call['year'],
                call['month'],
                call['day'],
                call['full_text'],
                call['summary'],
                json.dumps(call['tags'], ensure_ascii=False),
                call['text_length'],
                call['source_file'],
                call.get('drive_path', '')
            ))

            # –í—Å—Ç–∞–≤–ª—è–µ–º —Ç–µ–≥–∏
            for tag in call['tags']:
                cursor.execute(
                    "INSERT INTO call_tags (call_id, tag) VALUES (?, ?)",
                    (call['id'], tag)
                )

        self.conn.commit()

        source = "Google Drive" if self.drive_path else "–ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–∏"
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ in-memory SQLite ({len(calls)} –∑–∞–ø–∏—Å–µ–π –∏–∑ {source})")
        return self.conn

    @contextmanager
    def get_cursor(self):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –∫—É—Ä—Å–æ—Ä–∞"""
        if self.conn is None:
            self.setup_in_memory_db()

        cursor = self.conn.cursor()
        try:
            yield cursor
        finally:
            cursor.close()


# ==================== DeepSeek Planner ====================

class DeepSeekPlanner:
    """LLM –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∑–∞–ø—Ä–æ—Å–æ–≤"""

    def __init__(self, model, datasphere_node_url=None, drive_path=None):
        self.is_local = False  #isinstance(model, Llama)

        self.drive_path = drive_path
        self.available_tags = self._load_available_tags()

        if self.is_local:
            self.model = model
            self.model_name = 'local'
        elif datasphere_node_url:
            self.client = ollama.Client(host=datasphere_node_url, timeout=300)
            self.model_name = 'from_yandex_node'
            print(f"Mode: Yandex DataSphere (node url: {datasphere_node_url})")
        else:
            self.model_name = model
            self._setup_ollama_client()



    def _setup_ollama_client(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –∫–ª–∏–µ–Ω—Ç Ollama —Å —É—á–µ—Ç–æ–º Google Drive"""
        try:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è Colab
            host = "http://localhost:11434"

            # –ï—Å–ª–∏ –µ—Å—Ç—å Google Drive, –º–æ–∂–Ω–æ –∫—ç—à–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏
            if self.drive_path:
                models_cache_dir = os.path.join(self.drive_path, "models_cache")
                os.makedirs(models_cache_dir, exist_ok=True)
                print(f"üåê –ö—ç—à –º–æ–¥–µ–ª–µ–π Ollama –≤ Google Drive: {models_cache_dir}")

            self.client = ollama.Client(host=host, timeout=60.0)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
            try:
                self.client.list()
                print(f"‚úÖ Ollama –ø–æ–¥–∫–ª—é—á–µ–Ω, –º–æ–¥–µ–ª—å: {self.model_name}")
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")
                print("‚ÑπÔ∏è  –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω –≤ Colab")

        except ImportError:
            print("‚ùå Ollama –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            raise

    def _load_available_tags(self) -> List[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–µ–≥–∏ –∏–∑ JSON —Ñ–∞–π–ª–æ–≤"""
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –¥–∞–Ω–Ω—ã—Ö
        return [
            "–Ω–∏–∑–∫–æ–µ_–∫–∞—á–µ—Å—Ç–≤–æ_—Å—Ç–∏—Ä–∫–∏_–∏–ª–∏_—á–∏—Å—Ç–∫–∏",
            "–Ω–µ_–∑–∞–º–µ–Ω–∏–ª–∏_–∫–æ–≤—Ä—ã_–≤–æ–≤—Ä–µ–º—è",
            "–∫–ª–∏–µ–Ω—Ç_—Ö–æ—á–µ—Ç_–¥–æ–±–∞–≤–∏—Ç—å_–∫–æ–≤—Ä—ã",
            "–∫–ª–∏–µ–Ω—Ç_—Ö–æ—á–µ—Ç_–º–µ–Ω—å—à–µ_–∫–æ–≤—Ä–æ–≤",
            "–ø–æ–≥–∞—à–µ–Ω–∏–µ_–¥–æ–ª–≥–∞",
            "—Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏–µ_–¥–æ–≥–æ–≤–æ—Ä–∞",
            "–≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ_—É—Å–ª—É–≥",
            "–¥–æ–ª–≥–æ_–Ω–µ—Ç_–æ—Ç–≤–µ—Ç–∞_–Ω–∞_–∑–∞—è–≤–∫—É",
            "–ª–∏—à–Ω—è—è_–¥–æ—Å—Ç–∞–≤–∫–∞",
            "–¥–æ—Å—Ç–∞–≤–∏–ª–∏_–Ω–µ_—Ç–µ_–∫–æ–≤—Ä—ã",
            "–Ω–µ_–≤—ã—Å—Ç–∞–≤–ª–µ–Ω_–≤–æ–≤—Ä–µ–º—è_—Å—á–µ—Ç",
            "–Ω–µ–≤–µ—Ä–Ω–∞—è_—Å—É–º–º–∞_–≤_—Å—á–µ—Ç–µ",
            "–∫–æ–≤–µ—Ä_–∑–∞–±—Ä–∞–ª–∏_–±–µ–∑_–ø—Ä–∏—á–∏–Ω—ã",
            "–∑–∞–±—Ä–∞–ª–∏_–Ω–µ_—Ç–æ—Ç_–∫–æ–≤–µ—Ä",
            "–º–µ–Ω–µ–¥–∂–µ—Ä_–Ω–∞–≥—Ä—É–±–∏–ª_–∫–ª–∏–µ–Ω—Ç—É",
            "–Ω–µ–æ–ø—Ä–∞–≤–¥–∞–Ω–Ω–æ_–≤—ã—Å–æ–∫–∏–µ_—Ü–µ–Ω—ã",
            "–Ω–µ–æ–ø—Ä–∞–≤–¥–∞–Ω–Ω—ã–π_—Ä–æ—Å—Ç_—Ü–µ–Ω",
            "–Ω–æ–≤—ã–π_–∫–ª–∏–µ–Ω—Ç_–∑–∞–∫–ª—é—á–µ–Ω–∏–µ_–¥–æ–≥–æ–≤–æ—Ä–∞",
            "–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è_–∏–ª–∏_—É—Ç–æ—á–Ω–µ–Ω–∏–µ_–¥–µ—Ç–∞–ª–µ–π",
            "–ø–æ–º–µ–Ω—è—Ç—å_—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏",
            "–º–µ–Ω–µ–¥–∂–µ—Ä_–æ–±–µ—â–∞–ª_–Ω–æ_–Ω–µ_—Å–≤—è–∑–∞–ª—Å—è_—Å_–∫–ª–∏–µ–Ω—Ç–æ–º",
            "–∫–ª–∏–µ–Ω—Ç_—É—Ö–æ–¥–∏—Ç_–∫_–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º",
            "–ø—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å_—É—Å–ª—É–≥–∏",
            "–æ—à–∏–±–∫–∞_–≤_–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö"
        ]

    def create_analysis_plan(self, user_query: str) -> AnalysisPlan:
        """–°–æ–∑–¥–∞–µ—Ç –ø–ª–∞–Ω –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""

        prompt = self._build_planner_prompt(user_query)

        try:
            if self.is_local:
                response = self.model(
                    prompt,
                    max_tokens=500,
                    temperature=0.1)
            else:
                print('Use timeout 250')
                response = self.client.generate(
                    model=self.model_name,
                    prompt=prompt,
                    format="json",
                    options={'temperature': 0.1, 'num_predict': 250, 'timeout': 250}
                )

            plan_data = json.loads(response['response'])

            # –ü–∞—Ä—Å–∏–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–µ—Ä–∏–æ–¥
            time_period = self._parse_time_period(plan_data.get('time_period', {}))

            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º —Ç–µ–≥–∏
            target_tags = self._validate_tags(plan_data.get('target_tags', []))

            # –ü–∞—Ä—Å–∏–º –º–µ—Ç—Ä–∏–∫–∏
            metrics = self._parse_metrics(plan_data.get('metrics', []))

            return AnalysisPlan(
                time_period=time_period,
                target_tags=target_tags,
                metrics=metrics,
                grouping=plan_data.get('grouping', 'month'),
                comparison_tags=plan_data.get('comparison_tags', []),
                additional_filters=plan_data.get('filters', {})
            )

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–ª–∞–Ω –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            return self._create_default_plan(user_query)

    def _build_planner_prompt(self, user_query: str) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞"""
        current_date = datetime.now().strftime("%Y-%m-%d")

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∏—Å—Ç–æ—á–Ω–∏–∫–µ –¥–∞–Ω–Ω—ã—Ö
        data_source = "Google Drive" if self.drive_path else "–ª–æ–∫–∞–ª—å–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"

        return f"""–¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ –±–∞–∑—ã —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã—Ö –∑–≤–æ–Ω–∫–æ–≤ –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ –∞—Ä–µ–Ω–¥–µ –∫–æ–≤—Ä–æ–≤.

–ò–°–¢–û–ß–ù–ò–ö –î–ê–ù–ù–´–•: {data_source}

–ó–ê–ü–†–û–°: "{user_query}"

–¢–í–û–Ø –ó–ê–î–ê–ß–ê: –°–æ–∑–¥–∞—Ç—å –ø–ª–∞–Ω –∞–Ω–∞–ª–∏–∑–∞.
–°–∏—Å—Ç–µ–º–∞ –±—É–¥–µ—Ç –æ–±—Ä–∞—â–∞—Ç—å—Å—è –ø–æ —Ç–≤–æ–µ–º—É –ø–ª–∞–Ω—É –∫ —Ç–µ–∫—Å—Ç–∞–º —Å –∑–∞–ø–∏—Å—è–º–∏ —Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã—Ö –∑–≤–æ–Ω–∫–æ–≤ –∫–ª–∏–µ–Ω—Ç–æ–≤ –∑–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ª–µ—Ç, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º–∏ –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã–µ —Ç–µ–≥–∏ –∫–∞–∂–¥–æ–≥–æ –∑–≤–æ–Ω–∫–∞.

–î–û–°–¢–£–ü–ù–´–ï –¢–ï–ì–ò:
{', '.join(self.available_tags)}

–ú–ï–¢–†–ò–ö–ò, –∫–æ—Ç–æ—Ä—ã–µ —Å–∏—Å—Ç–µ–º–∞ –º–æ–∂–µ—Ç –ø–æ—Å—á–∏—Ç–∞—Ç—å –¥–ª—è —Ç–µ–±—è –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –∑–∞–ø—Ä–æ—Å, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ:
1. count_by_tag - –ø–æ–¥—Å—á–µ—Ç –∑–≤–æ–Ω–∫–æ–≤ —Å –∑–∞–¥–∞–Ω–Ω—ã–º —Ç–µ–≥–æ–º –∑–∞ –ø–µ—Ä–∏–æ–¥
2. top_n_tags - —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Ç–µ–≥–∏ –∑–≤–æ–Ω–∫–æ–≤ –∑–∞ –ø–µ—Ä–∏–æ–¥
3. tag_trends - –¥–∏–Ω–∞–º–∏–∫–∞ —Ç–µ–≥–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏: —Å—Ç–∞–ª –ª–∏ —á–∞—â–µ –∏–ª–∏ —Ä–µ–∂–µ –≤—Å—Ç—Ä–µ—á–∞—Ç—å—Å—è –∑–∞ –ø–µ—Ä–∏–æ–¥?
–°–µ–≥–æ–¥–Ω—è—à–Ω—è—è –¥–∞—Ç–∞: {current_date} - –∏—Å–ø–æ–ª—å–∑—É–π –µ–µ, —á—Ç–æ–±—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–µ—Ä–∏–æ–¥ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–ª—É—á–∞–µ, –µ—Å–ª–∏ –≤ –∑–∞–ø—Ä–æ—Å–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–µ—Ä–∏–æ–¥ —É–∫–∞–∑–∞–Ω –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å–µ–≥–æ–¥–Ω—è—à–Ω–µ–≥–æ –¥–Ω—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–≤ –ø—Ä–æ—à–ª–æ–º –≥–æ–¥—É" –∏ —Ç.–ø.)

–í–ï–†–ù–ò JSON —Å –ø–ª–∞–Ω–æ–º —Ç–æ–≥–æ, —á—Ç–æ —Å–∏—Å—Ç–µ–º–µ –Ω—É–∂–Ω–æ –∏–∑–≤–ª–µ—á—å –∏–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –∑–∞–ø—Ä–æ—Å, –∞ –∏–º–µ–Ω–Ω–æ: –∑–∞ –∫–∞–∫–æ–π –ø–µ—Ä–∏–æ–¥ –ø–æ–Ω–∞–¥–æ–±—è—Ç—Å—è –¥–∞–Ω–Ω—ã–µ? –ü–æ –∫–∞–∫–∏–º –∏–º–µ–Ω–æ —Ç–µ–≥–∞–º –≤—ã–±–∏—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –¥–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å? –ö–∞–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ–¥—Å—á–∏—Ç–∞—Ç—å –ø–æ —ç—Ç–∏–º –¥–∞–Ω–Ω—ã–º –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –¥–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å?

{{
  "time_period": {{
    "description": "–æ–ø–∏—Å–∞–Ω–∏–µ –ø–µ—Ä–∏–æ–¥–∞",
    "start": "YYYY-MM-DD –∏–ª–∏ null",
    "end": "YYYY-MM-DD –∏–ª–∏ null"
  }},
  "target_tags": ["—Ç–µ–≥1", "—Ç–µ–≥2", ... (1 or more tags)],
  "metrics": ["count_by_tag" and/or "tag_trends" and/or "top_n_tags" (necessary metrics)],
  "grouping": "month/week/day"
  }}

–û—Ç–≤–µ—Ç:
<|think|>false<|end|>"""

    def _parse_time_period(self, period_data: Dict) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–µ—Ä–∏–æ–¥"""
        today = datetime.now()
        description = period_data.get('description', '')

        # –ù–∞—á–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        start = today - timedelta(days=30)  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Å—è—Ü
        end = today

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–µ—Ä–∏–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø–∏—Å–∞–Ω–∏—è
        if '–ø–æ—Å–ª–µ–¥–Ω–∏–µ 6 –º–µ—Å—è—Ü–µ–≤' in description.lower():
            start = today - timedelta(days=30 * 6)
        elif '—ç—Ç–æ—Ç –º–µ—Å—è—Ü' in description.lower():
            start = datetime(today.year, today.month, 1)
        elif '—ç—Ç–æ—Ç –≥–æ–¥' in description.lower():
            start = datetime(today.year, 1, 1)
        elif '–ø–µ—Ä–≤—ã–π –∫–≤–∞—Ä—Ç–∞–ª 2024' in description.lower():
            start = datetime(2024, 1, 1)
            end = datetime(2024, 3, 31)
        elif '–ø—Ä–æ—à–ª—ã–π –≥–æ–¥' in description.lower():
            start = datetime(today.year - 1, 1, 1)
            end = datetime(today.year - 1, 12, 31)

        # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã —Ç–æ—á–Ω—ã–µ –¥–∞—Ç—ã
        if period_data.get('start'):
            try:
                start = datetime.fromisoformat(period_data['start'])
            except:
                pass
        if period_data.get('end'):
            try:
                end = datetime.fromisoformat(period_data['end'])
            except:
                pass

        return {
            'start': start,
            'end': end,
            'description': description or f"—Å {start.strftime('%d.%m.%Y')} –ø–æ {end.strftime('%d.%m.%Y')}"
        }

    def _validate_tags(self, tags: List[str]) -> List[str]:
        """–§–∏–ª—å—Ç—Ä—É–µ—Ç –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–≥–∏"""
        valid_tags = []
        for tag in tags:
            # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ —Ç–µ–≥–∏
            for available_tag in self.available_tags:
                if tag.lower() in available_tag.lower() or available_tag.lower() in tag.lower():
                    valid_tags.append(available_tag)
                    break

        return valid_tags or ['–Ω–∏–∑–∫–æ–µ_–∫–∞—á–µ—Å—Ç–≤–æ_—Å—Ç–∏—Ä–∫–∏_–∏–ª–∏_—á–∏—Å—Ç–∫–∏']  # Fallback

    def _parse_metrics(self, metrics: List[str]) -> List[MetricType]:
        """–ü–∞—Ä—Å–∏—Ç –º–µ—Ç—Ä–∏–∫–∏"""
        metric_map = {
            'count_by_tag': MetricType.COUNT_BY_TAG,
            'top_n_tags': MetricType.TOP_N_TAGS,
            'tag_trends': MetricType.TAG_TRENDS,
            'comparison': MetricType.COMPARISON
        }

        result = []
        for metric in metrics:
            if metric in metric_map:
                result.append(metric_map[metric])

        return result or [MetricType.COUNT_BY_TAG]

    def _create_default_plan(self, user_query: str) -> AnalysisPlan:
        """–°–æ–∑–¥–∞–µ—Ç –ø–ª–∞–Ω –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
        today = datetime.now()

        # –≠–≤—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–≥–∞
        target_tags = []
        if '–∫–∞—á–µ—Å—Ç–≤' in user_query.lower() or '—Å—Ç–∏—Ä–∫' in user_query.lower():
            target_tags.append('–Ω–∏–∑–∫–æ–µ_–∫–∞—á–µ—Å—Ç–≤–æ_—Å—Ç–∏—Ä–∫–∏_–∏–ª–∏_—á–∏—Å—Ç–∫–∏')
        if '—Ü–µ–Ω' in user_query.lower() or '–¥–æ—Ä–æ–≥' in user_query.lower():
            target_tags.append('–Ω–µ–æ–ø—Ä–∞–≤–¥–∞–Ω–Ω–æ_–≤—ã—Å–æ–∫–∏–µ_—Ü–µ–Ω—ã')
        if '–∫–æ–Ω—Å—É–ª—å—Ç' in user_query.lower() or '—É—Ç–æ—á–Ω' in user_query.lower():
            target_tags.append('–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è_–∏–ª–∏_—É—Ç–æ—á–Ω–µ–Ω–∏–µ_–¥–µ—Ç–∞–ª–µ–π')

        return AnalysisPlan(
            time_period={
                'start': today - timedelta(days=30 * 6),
                'end': today,
                'description': '–ø–æ—Å–ª–µ–¥–Ω–∏–µ 6 –º–µ—Å—è—Ü–µ–≤'
            },
            target_tags=target_tags or ['–Ω–∏–∑–∫–æ–µ_–∫–∞—á–µ—Å—Ç–≤–æ_—Å—Ç–∏—Ä–∫–∏_–∏–ª–∏_—á–∏—Å—Ç–∫–∏'],
            metrics=[MetricType.COUNT_BY_TAG, MetricType.TAG_TRENDS],
            grouping='month'
        )


# ==================== Query Executor ====================

class JSONQueryExecutor:
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã –∫ JSON –¥–∞–Ω–Ω—ã–º"""

    def __init__(self, data_loader: DriveDataLoader):
        self.data_loader = data_loader

    def execute_plan(self, plan: AnalysisPlan) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–ª–∞–Ω –∞–Ω–∞–ª–∏–∑–∞"""

        results = {}

        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞ –ø–µ—Ä–∏–æ–¥
        all_calls = self.data_loader.load_all_calls()

        if not all_calls:
            print("‚ö†Ô∏è  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            return {
                'error': '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞',
                'summary_stats': {
                    'total_calls': 0,
                    'period': plan.time_period['description'],
                    'date_range': f"{plan.time_period['start'].strftime('%Y-%m-%d')} - {plan.time_period['end'].strftime('%Y-%m-%d')}"
                }
            }

        print(f'{len(all_calls)} –∑–≤–æ–Ω–∫–æ–≤ –≤—Å–µ–≥–æ')
        filtered_calls = self._filter_calls_by_period(all_calls, plan.time_period)
        print(f'{len(filtered_calls)} –∑–≤–æ–Ω–∫–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –ø–µ—Ä–∏–æ–¥—É')

        # –í—ã–ø–æ–ª–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        for metric in plan.metrics:
            if metric == MetricType.COUNT_BY_TAG:
                results['count_by_tag'] = self._count_by_tag(filtered_calls, plan.target_tags)

            elif metric == MetricType.TAG_TRENDS:
                results['tag_trends'] = self._tag_trends(
                    filtered_calls,
                    plan.target_tags,
                    plan.grouping
                )

            elif metric == MetricType.TOP_N_TAGS:
                results['top_n_tags'] = self._top_n_tags(filtered_calls, n=5)

            elif metric == MetricType.COMPARISON:
                results['comparison'] = self._compare_tags(
                    filtered_calls,
                    plan.comparison_tags or plan.target_tags[:2]
                )

        # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        results['summary_stats'] = {
            'total_calls': len(filtered_calls),
            'period': plan.time_period['description'],
            'date_range': f"{plan.time_period['start'].strftime('%Y-%m-%d')} - {plan.time_period['end'].strftime('%Y-%m-%d')}",
            'data_source': 'Google Drive' if self.data_loader.drive_path else 'Local'
        }

        return results

    def _filter_calls_by_period(self, calls: List[Dict], period: Dict) -> List[Dict]:
        """–§–∏–ª—å—Ç—Ä—É–µ—Ç –∑–≤–æ–Ω–∫–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É –ø–µ—Ä–∏–æ–¥—É"""
        start_date = period['start']
        end_date = period['end']

        filtered = []
        for call in calls:
            call_date = call['call_date']
            if start_date <= call_date <= end_date:
                filtered.append(call)

        return filtered

    def _count_by_tag(self, calls: List[Dict], target_tags: List[str]) -> Dict[str, int]:
        """–ü–æ–¥—Å—á–µ—Ç –∑–≤–æ–Ω–∫–æ–≤ –ø–æ —Ç–µ–≥–∞–º"""
        counts = defaultdict(int)

        for call in calls:
            for tag in call['tags']:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–≤–ø–∞–¥–∞–µ—Ç –ª–∏ —Ç–µ–≥ —Å —Ü–µ–ª–µ–≤—ã–º–∏
                for target in target_tags:
                    if target.lower() in tag.lower() or tag.lower() in target.lower():
                        counts[target] += 1
                        break

        return dict(counts)

    def _tag_trends(self, calls: List[Dict], target_tags: List[str], grouping: str) -> Dict[str, List]:
        """–î–∏–Ω–∞–º–∏–∫–∞ —Ç–µ–≥–æ–≤ –ø–æ –≤—Ä–µ–º–µ–Ω–∏"""
        if not target_tags or not calls:
            return {}

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –º–µ—Å—è—Ü–∞–º/–Ω–µ–¥–µ–ª—è–º
        trends = defaultdict(lambda: defaultdict(int))

        for call in calls:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª—é—á –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
            if grouping == 'month':
                period_key = call['call_date'].strftime('%Y-%m')
            elif grouping == 'week':
                year, week, _ = call['call_date'].isocalendar()
                period_key = f"{year}-W{week:02d}"
            else:  # day
                period_key = call['call_date'].strftime('%Y-%m-%d')

            # –°—á–∏—Ç–∞–µ–º —Ç–µ–≥–∏
            for tag in call['tags']:
                for target in target_tags:
                    if target.lower() in tag.lower() or tag.lower() in target.lower():
                        trends[target][period_key] += 1
                        break

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–≥–∞
        result = {}
        for tag, period_counts in trends.items():
            result[tag] = [
                {'period': period, 'count': count}
                for period, count in sorted(period_counts.items())
            ]

        return result

    def _top_n_tags(self, calls: List[Dict], n: int = 5) -> List[Dict]:
        """–¢–æ–ø-N —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Ç–µ–≥–æ–≤"""
        tag_counter = Counter()

        for call in calls:
            tag_counter.update(call['tags'])

        return [
            {'tag': tag, 'count': count}
            for tag, count in tag_counter.most_common(n)
        ]

    def _compare_tags(self, calls: List[Dict], tags: List[str]) -> Dict[str, Any]:
        """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –¥–≤–∞ —Ç–µ–≥–∞"""
        if len(tags) < 2:
            tags = tags + [None] * (2 - len(tags))

        counts = self._count_by_tag(calls, tags[:2])

        return {
            'tag1': {'name': tags[0], 'count': counts.get(tags[0], 0)},
            'tag2': {'name': tags[1], 'count': counts.get(tags[1], 0)},
            'total_calls': len(calls),
            'ratio': counts.get(tags[0], 0) / counts.get(tags[1], 1) if counts.get(tags[1], 0) > 0 else 0
        }


# ==================== DeepSeek Analyzer ====================

class DeepSeekAnalyzer:
    """LLM –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤"""

    def __init__(self, model, datasphere_node_url = None, drive_path: str = None):
        self.is_local = False # isinstance(model, Llama)

        if self.is_local:
            self.model_name = 'local'
            self.model = model
        elif datasphere_node_url:
            self.client = ollama.Client(host=datasphere_node_url, timeout=300)
            self.model_name = 'from_yandex_node'
            print(f"Mode: Yandex DataSphere (node url: {datasphere_node_url})")
        else:
            self.model_name = model
            try:
                self.client = ollama.Client(
                    host="http://localhost:11434",
                    timeout=90.0  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π
                )
            except ImportError:
                print("‚ùå Ollama –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
                raise

        self.drive_path = drive_path



    def generate_answer(self, user_query: str, results: Dict, plan: AnalysisPlan) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""

        prompt = self._build_analyzer_prompt(user_query, results, plan)

        try:
            if self.is_local:
                response = self.model(prompt,
                                      temperature=0.3,
                                      num_predict=1000)
            else:
                response = self.client.generate(
                    model=self.model_name,
                    prompt=prompt,
                    options={'temperature': 0.3, 'num_predict': 1000}
                )

            return response['response'].strip()

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞: {e}")
            return self._generate_fallback_answer(results, plan)

    def _build_analyzer_prompt(self, user_query: str, results: Dict, plan: AnalysisPlan) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""

        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
        results_str = json.dumps(results, ensure_ascii=False, indent=2, default=str)

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–µ
        data_source = "Google Drive" if self.drive_path else "–ª–æ–∫–∞–ª—å–Ω–æ–π –±–∞–∑—ã"

        return f"""–¢—ã ‚Äî —Å—Ç–∞—Ä—à–∏–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ –∞—Ä–µ–Ω–¥–µ –∫–æ–≤—Ä–æ–≤.

–ò–°–¢–û–ß–ù–ò–ö –î–ê–ù–ù–´–•: –ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {data_source}

–ó–ê–ü–†–û–° –ö–õ–ò–ï–ù–¢–ê: "{user_query}"

–î–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –∑–∞–ø—Ä–æ—Å —Å–∏—Å—Ç–µ–º–∞ –≤—ã–±—Ä–∞–ª–∞ —Ç–µ–∫—Å—Ç—ã –æ–±—Ä–∞—â–µ–Ω–∏–π –∫–ª–∏–µ–Ω—Ç–æ–≤ –∑–∞ –Ω—É–∂–Ω—ã–π –ø–µ—Ä–∏–æ–¥ –∏ –ø–æ—Å—á–∏—Ç–∞–ª–∞ –Ω—É–∂–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏.
- –ü–µ—Ä–∏–æ–¥, –∫–æ—Ç–æ—Ä—ã–º –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–ª—Å—è –∫–ª–∏–µ–Ω—Ç: {plan.time_period['description']}
- –ü–æ–¥—Ö–æ–¥—è—â–∏–µ —Ç–µ–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–±—Ä–∞–ª–∞ —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ –æ–±—Ä–∞—â–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {', '.join(plan.target_tags)}
- –ú–µ—Ç—Ä–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å–∏—Å—Ç–µ–º–∞ –ø–æ–¥—Å—á–∏—Ç–∞–ª–∞ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞, –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤ –æ–±—Ä–∞—â–µ–Ω–∏–π, –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø–æ —ç—Ç–∏–º —Ç–µ–≥–∞–º: {[m.value for m in plan.metrics]}

–í–æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–¥–∞–ª–∞ —Å–∏—Å—Ç–µ–º–∞ –ø–æ –ø–æ–¥—Å—á–µ—Ç–∞–º –º–µ—Ç—Ä–∏–∫ –¥–ª—è —ç—Ç–∏—Ö —Ç–µ–≥–æ–≤:
{results_str}

–¢–í–û–Ø –ó–ê–î–ê–ß–ê:
1. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ü–∏—Ñ—Ä—ã –≤ —ç—Ç–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö (–µ—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ –ø—É—Å—Ç–æ–π!)
2. –û—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –∑–∞–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞
3. –í—ã–¥–µ–ª–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã
4. –ì–æ–≤–æ—Ä–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ, —Å —Ü–∏—Ñ—Ä–∞–º–∏

–§–û–†–ú–ê–¢:
- –ö—Ä–∞—Ç–∫–∏–π –≤—ã–≤–æ–¥
- –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)

–ï—Å–ª–∏ —Ç—ã –≤–∏–¥–∏—à—å, —á—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –¥–∞–ª–∞ —Ç–µ–±–µ –ø—É—Å—Ç—ã–µ –º–µ—Ç—Ä–∏–∫–∏, –∏–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –Ω–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –∑–∞–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞, - —Ç–∞–∫ –∏ –Ω–∞–ø–∏—à–∏.

–û–¢–í–ï–¢ –ù–ê –†–£–°–°–ö–û–ú:"""

    def _generate_fallback_answer(self, results: Dict, plan: AnalysisPlan) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –µ—Å–ª–∏ LLM –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞"""

        answer_parts = []

        # –ö—Ä–∞—Ç–∫–∏–π –≤—ã–≤–æ–¥
        answer_parts.append(f"üìä –ê–Ω–∞–ª–∏–∑ –∑–∞ –ø–µ—Ä–∏–æ–¥: {plan.time_period['description']}")

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ —Ç–µ–≥–∞–º
        if 'count_by_tag' in results and results['count_by_tag']:
            answer_parts.append("\nüìà –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–≤–æ–Ω–∫–æ–≤ –ø–æ —Ç–µ–≥–∞–º:")
            for tag, count in results['count_by_tag'].items():
                answer_parts.append(f"  ‚Ä¢ {tag}: {count}")
        else:
            answer_parts.append("\n‚ö†Ô∏è  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ —É–∫–∞–∑–∞–Ω–Ω—ã–º —Ç–µ–≥–∞–º")

        # –î–∏–Ω–∞–º–∏–∫–∞
        if 'tag_trends' in results:
            for tag, trends in results['tag_trends'].items():
                if trends:
                    first = trends[0]['count']
                    last = trends[-1]['count']
                    change = ((last - first) / first * 100) if first > 0 else 0
                    trend_desc = "üìà —Ä–æ—Å—Ç" if change > 0 else "üìâ —Å–Ω–∏–∂–µ–Ω–∏–µ" if change < 0 else "‚û°Ô∏è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π"
                    answer_parts.append(f"\nüìÖ –î–∏–Ω–∞–º–∏–∫–∞ '{tag}': {trend_desc} ({abs(change):.1f}%)")

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if 'count_by_tag' in results and results['count_by_tag']:
            max_tag = max(results['count_by_tag'].items(), key=lambda x: x[1])[0] if results['count_by_tag'] else None
            if max_tag and ('–∂–∞–ª–æ–±–∞' in max_tag or '–Ω–∏–∑–∫–æ–µ' in max_tag):
                answer_parts.append(
                    f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Ç–µ–≥ '{max_tag}' - —ç—Ç–æ —Å–∞–º–∞—è —á–∞—Å—Ç–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è –æ–±—Ä–∞—â–µ–Ω–∏–π")

        return "\n".join(answer_parts)


# ==================== –ì–ª–∞–≤–Ω–∞—è MCP —Å–∏—Å—Ç–µ–º–∞ ====================

class JSONCallAnalyticsMCP:
    """–ì–ª–∞–≤–Ω–∞—è MCP —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Google Drive JSON —Ñ–∞–π–ª–∞–º–∏"""

    def __init__(self, json_directory: str, model, node_url=None, drive_path: str = None):
        self.drive_path = drive_path
        self.data_loader = DriveDataLoader(json_directory, drive_path)
        self.planner = DeepSeekPlanner(model, node_url, drive_path)
        self.executor = JSONQueryExecutor(self.data_loader)
        self.analyzer = DeepSeekAnalyzer(model, node_url, drive_path)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        print("üìÇ –ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–æ–≤...")
        self.total_calls = len(self.data_loader.load_all_calls())

        if self.total_calls == 0:
            print("‚ö†Ô∏è  –í–Ω–∏–º–∞–Ω–∏–µ: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            if self.drive_path:
                print(f"‚ÑπÔ∏è  –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ JSON —Ñ–∞–π–ª–æ–≤ –≤ Google Drive: {json_directory}")
        else:
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {self.total_calls} –∑–≤–æ–Ω–∫–æ–≤")
            if self.drive_path:
                print(f"üåê –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ Google Drive")

    def process_query(self, user_query: str) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""

        print(f"\nüîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∑–∞–ø—Ä–æ—Å: '{user_query}'")

        if self.drive_path:
            print(f"üåê –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö: Google Drive")

        # 1. –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ (LLM)
        print("ü§ñ –°–æ–∑–¥–∞—é –ø–ª–∞–Ω –∞–Ω–∞–ª–∏–∑–∞...")
        analysis_plan = self.planner.create_analysis_plan(user_query)

        print(f"   üìÖ –ü–µ—Ä–∏–æ–¥: {analysis_plan.time_period['description']}, {analysis_plan.time_period['start']}, {analysis_plan.time_period['end']}")
        print(f"   üè∑Ô∏è  –¢–µ–≥–∏: {', '.join(analysis_plan.target_tags)}")
        print(f"   üìä –ú–µ—Ç—Ä–∏–∫–∏: {[m.value for m in analysis_plan.metrics]}")

        # 2. –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞
        print("üìä –í—ã–ø–æ–ª–Ω—è—é –∞–Ω–∞–ª–∏–∑...")
        analysis_results = self.executor.execute_plan(analysis_plan)

        # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (LLM)
        print("üí≠ –§–æ—Ä–º—É–ª–∏—Ä—É—é –æ—Ç–≤–µ—Ç...")
        answer = self.analyzer.generate_answer(user_query, analysis_results, analysis_plan)

        # 4. –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç
        response = {
            'query': user_query,
            'analysis_plan': analysis_plan.to_dict(),
            'raw_results': analysis_results,
            'answer': answer,
            'total_calls_analyzed': analysis_results.get('summary_stats', {}).get('total_calls', 0),
            'processing_time': datetime.now().isoformat(),
            'model_used': self.planner.model_name,
            'data_source': 'Google Drive' if self.drive_path else 'Local'
        }

        # 5. –í—ã–≤–æ–¥–∏–º –∫—Ä–∞—Ç–∫—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self._print_analysis_summary(analysis_results)

        return response

    def _print_analysis_summary(self, results: Dict[str, Any]):
        """–í—ã–≤–æ–¥–∏—Ç –∫—Ä–∞—Ç–∫—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∞–Ω–∞–ª–∏–∑–∞"""
        print("üìà –ö–†–ê–¢–ö–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print("-" * 40)

        if 'summary_stats' in results:
            stats = results['summary_stats']
            print(f"üìÖ –ü–µ—Ä–∏–æ–¥: {stats.get('period', 'N/A')}")
            print(f"üìû –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –∑–≤–æ–Ω–∫–æ–≤: {stats.get('total_calls', 0)}")
            print(f"üìç –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö: {stats.get('data_source', 'Local')}")

        if 'count_by_tag' in results:
            counts = results['count_by_tag']
            if counts:
                print("\nüìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ —Ç–µ–≥–∞–º:")
                for tag, count in counts.items():
                    print(f"  ‚Ä¢ {tag}: {count}")
            else:
                print("\n‚ö†Ô∏è  –ù–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ —É–∫–∞–∑–∞–Ω–Ω—ã–º —Ç–µ–≥–∞–º")

        if 'top_n_tags' in results and results['top_n_tags']:
            print("\nüèÜ –¢–æ–ø —Ç–µ–≥–∏:")
            for i, item in enumerate(results['top_n_tags'][:3], 1):
                print(f"  {i}. {item['tag']}: {item['count']}")

        if 'tag_trends' in results:
            for tag, trends in results['tag_trends'].items():
                if trends and len(trends) >= 2:
                    first = trends[0]['count']
                    last = trends[-1]['count']
                    change = ((last - first) / first * 100) if first > 0 else 0
                    trend_icon = "üìà" if change > 0 else "üìâ" if change < 0 else "‚û°Ô∏è"
                    print(f"\nüìÖ –î–∏–Ω–∞–º–∏–∫–∞ '{tag}': {trend_icon} {abs(change):.1f}%")

        print("-" * 40)

    def get_system_info(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∏—Å—Ç–µ–º–µ"""
        calls = self.data_loader.load_all_calls()

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–µ–≥–∏
        all_tags = []
        for call in calls:
            all_tags.extend(call['tags'])

        unique_tags = set(all_tags)

        # –î–∞—Ç—ã
        dates = [call['call_date'] for call in calls]

        return {
            'total_calls': len(calls),
            'unique_tags_count': len(unique_tags),
            'date_range': {
                'start': min(dates).isoformat() if dates else None,
                'end': max(dates).isoformat() if dates else None
            },
            'average_text_length': sum(len(c['full_text']) for c in calls) // len(calls) if calls else 0,
            'model': self.planner.model_name,
            'data_source': 'Google Drive' if self.drive_path else 'Local Files',
            'drive_path': self.drive_path if self.drive_path else None
        }

    def test_system(self) -> bool:
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã"""
        print("üß™ –¢–µ—Å—Ç–∏—Ä—É—é —Å–∏—Å—Ç–µ–º—É...")

        try:
            # –¢–µ—Å—Ç 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            calls = self.data_loader.load_all_calls(limit=10)
            if not calls:
                print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
                if self.drive_path:
                    print(f"‚ÑπÔ∏è  –ü—Ä–æ–≤–µ—Ä—å—Ç–µ Google Drive: {self.data_loader.csv_dir}")
                return False

            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(calls)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π")

            # –¢–µ—Å—Ç 2: –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
            test_query = "–¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å: –∂–∞–ª–æ–±—ã –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ"
            plan = self.planner.create_analysis_plan(test_query)
            if not plan.target_tags:
                print("‚ùå –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –Ω–µ –≤–µ—Ä–Ω—É–ª —Ç–µ–≥–∏")
                return False

            print(f"‚úÖ –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç, –≤—ã–±—Ä–∞–Ω—ã —Ç–µ–≥–∏: {plan.target_tags}")

            # –¢–µ—Å—Ç 3: –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
            results = self.executor.execute_plan(plan)
            if 'summary_stats' not in results:
                print("‚ùå –ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
                return False

            print(f"‚úÖ –ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª {results['summary_stats'].get('total_calls', 0)} –∑–≤–æ–Ω–∫–æ–≤")

            # –¢–µ—Å—Ç 4: –ê–Ω–∞–ª–∏–∑
            answer = self.analyzer.generate_answer(test_query, results, plan)
            if not answer or len(answer) < 10:
                print("‚ùå –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª –æ—Ç–≤–µ—Ç")
                return False

            print(f"‚úÖ –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª –æ—Ç–≤–µ—Ç –¥–ª–∏–Ω–æ–π {len(answer)} —Å–∏–º–≤–æ–ª–æ–≤")

            print("\nüéâ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
            import traceback
            traceback.print_exc()
            return False