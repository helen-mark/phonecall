import json
import librosa
import numpy as np
import whisper
import os

#warnings.filterwarnings('ignore')
import soundfile as sf
from pydub import AudioSegment

import noisereduce as nr


def get_audio_info(file_path):
    """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞—É–¥–∏–æ—Ñ–∞–π–ª–µ"""

    print(f"–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞: {file_path}")
    print("=" * 40)

    # –°–ø–æ—Å–æ–± 1: —á–µ—Ä–µ–∑ librosa (–ª—É—á—à–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞)
    try:
        y, sr = librosa.load(file_path, sr=None)  # sr=None - –∑–∞–≥—Ä—É–∂–∞–µ–º —Å –∏—Å—Ö–æ–¥–Ω–æ–π —á–∞—Å—Ç–æ—Ç–æ–π
        print(f"üìä Librosa: {sr} Hz")
        print(f"üìè –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {len(y) / sr:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"üéµ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤: {y.ndim}")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ librosa: {e}")

    print("-" * 20)

    # –°–ø–æ—Å–æ–± 2: —á–µ—Ä–µ–∑ soundfile
    try:
        info = sf.info(file_path)
        print(f"üìä SoundFile: {info.samplerate} Hz")
        print(f"üìè –ö–∞–¥—Ä–æ–≤: {info.frames}")
        print(f"üéµ –ö–∞–Ω–∞–ª—ã: {info.channels}")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ soundfile: {e}")

    print("-" * 20)

    # –°–ø–æ—Å–æ–± 3: —á–µ—Ä–µ–∑ pydub
    try:
        audio = AudioSegment.from_file(file_path)
        print(f"üìä Pydub: {audio.frame_rate} Hz")
        print(f"üìè –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {len(audio) / 1000:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"üéµ –ö–∞–Ω–∞–ª—ã: {'–ú–æ–Ω–æ' if audio.channels == 1 else '–°—Ç–µ—Ä–µ–æ'}")
        print(f"üìù –†–∞–∑–º–µ—Ä —Å—ç–º–ø–ª–∞: {audio.sample_width} –±–∞–π—Ç–∞")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ pydub: {e}")




class AudioAnalyzer:
    def __init__(self, model_size):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –∞—É–¥–∏–æ
        model_size: "tiny", "base", "small", "medium", "large"
        """
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Whisper...")
        self.asr_model = whisper.load_model(model_size)
        print("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

    def extract_audio_features(self, audio_path):
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ç–æ–Ω–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∞—É–¥–∏–æ
        """
        print(f"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ {audio_path}...")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∞—É–¥–∏–æ
        y, sr = librosa.load(audio_path, sr=16000)

        features = {}

        # 1. –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∞—É–¥–∏–æ
        features['duration'] = len(y) / sr
        features['sample_rate'] = sr

        # 2. –ì—Ä–æ–º–∫–æ—Å—Ç—å (Energy)
        rms = librosa.feature.rms(y=y)
        features['loudness'] = {
            'mean': float(np.mean(rms)),
            'std': float(np.std(rms)),
            'max': float(np.max(rms)),
            'min': float(np.min(rms))
        }

        # 3. –í—ã—Å–æ—Ç–∞ —Ç–æ–Ω–∞ (Pitch)
        pitch, voiced_flag, voiced_probs = librosa.pyin(y=y, fmin=50, fmax=400, sr=sr)
        pitch_values = pitch[~np.isnan(pitch)]

        if len(pitch_values) > 0:
            features['pitch'] = {
                'mean': float(np.mean(pitch_values)),
                'std': float(np.std(pitch_values)),
                'max': float(np.max(pitch_values)),
                'min': float(np.min(pitch_values)),
                'range': float(np.max(pitch_values) - np.min(pitch_values))
            }
        else:
            features['pitch'] = {'mean': 0, 'std': 0, 'max': 0, 'min': 0, 'range': 0}

        # 4. –¢–µ–º–ø —Ä–µ—á–∏ (—á–µ—Ä–µ–∑ onset detection)
        tempo, beats = librosa.beat.beat_track(y=y, sr=sr)
        features['tempo'] = float(tempo)
        features['beat_frames'] = len(beats)

        # 5. –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ (MFCC –¥–ª—è —Ç–µ–º–±—Ä–∞)
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        features['mfcc_stats'] = {
            'mfcc_mean': [float(x) for x in np.mean(mfcc, axis=1)],
            'mfcc_std': [float(x) for x in np.std(mfcc, axis=1)]
        }

        # 6. Zero-crossing rate (–ø–æ–∫–∞–∑–∞—Ç–µ–ª—å —à—É–º–∞/—Ä–µ–∑–∫–æ—Å—Ç–∏)
        zcr = librosa.feature.zero_crossing_rate(y)
        features['zero_crossing_rate'] = {
            'mean': float(np.mean(zcr)),
            'std': float(np.std(zcr))
        }

        # 7. –ü–∞—É–∑—ã –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏—Ö–∏—Ö —É—á–∞—Å—Ç–∫–æ–≤ –∫–∞–∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –ø–∞—É–∑
        frame_length = 2048
        hop_length = 512
        rms_frames = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]
        silence_threshold = np.mean(rms_frames) * 0.3
        silent_frames = np.sum(rms_frames < silence_threshold)
        total_frames = len(rms_frames)

        features['pauses'] = {
            'silence_ratio': float(silent_frames / total_frames),
            'total_silent_frames': int(silent_frames),
            'silence_threshold': float(silence_threshold)
        }

        print("–ü—Ä–∏–∑–Ω–∞–∫–∏ —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω—ã!")
        return features

    def transcribe_audio(self, audio_path):
        """
        –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –∞—É–¥–∏–æ –≤ —Ç–µ–∫—Å—Ç
        """
        print(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è {audio_path}...")

        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é Whisper
        result = self.asr_model.transcribe(audio_path)

        transcription = {
            'text': result['text'],
            'language': result.get('language', 'ru'),
            'segments': []
        }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
        for segment in result.get('segments', []):
            transcription['segments'].append({
                'start': segment['start'],
                'end': segment['end'],
                'text': segment['text'],
                'confidence': segment.get('confidence', 0)
            })

        print("–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        return transcription

    def analyze_audio_file(self, audio_path, output_file=None):
        """
        –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞
        """
        print(f"\n=== –ù–∞—á–∞–ª–æ –∞–Ω–∞–ª–∏–∑–∞ {audio_path} ===")

        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è
        transcription = self.transcribe_audio(audio_path)

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        audio_features = self.extract_audio_features(audio_path)

        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        result = {
            'audio_file': audio_path,
            'transcription': transcription,
            'audio_features': audio_features,
            'summary': self._generate_summary(transcription, audio_features)
        }

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_file}")

        print("=== –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω ===")
        return result

    def _generate_summary(self, transcription, features):
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        """
        text_length = len(transcription['text'])
        duration = features['duration']

        # –ê–Ω–∞–ª–∏–∑ —Ç–µ–º–ø–∞ —Ä–µ—á–∏
        words_per_minute = (text_length / 6) / (duration / 60) if duration > 0 else 0

        # –ê–Ω–∞–ª–∏–∑ –∏–Ω—Ç–æ–Ω–∞—Ü–∏–∏
        pitch_variability = features['pitch']['std']
        loudness_variability = features['loudness']['std']

        summary = {
            'text_length': text_length,
            'audio_duration_seconds': round(duration, 2),
            'words_per_minute_approx': round(words_per_minute, 2),
            'speech_characteristics': []
        }

        # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ä–µ—á–∏
        if pitch_variability > 20:
            summary['speech_characteristics'].append("–≤—ã—Ä–∞–∂–µ–Ω–Ω–∞—è –∏–Ω—Ç–æ–Ω–∞—Ü–∏–æ–Ω–Ω–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å")
        else:
            summary['speech_characteristics'].append("—Ä–æ–≤–Ω–∞—è –∏–Ω—Ç–æ–Ω–∞—Ü–∏—è")

        if loudness_variability > 0.01:
            summary['speech_characteristics'].append("–ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –≥—Ä–æ–º–∫–æ—Å—Ç—å")
        else:
            summary['speech_characteristics'].append("—Å—Ç–∞–±–∏–ª—å–Ω–∞—è –≥—Ä–æ–º–∫–æ—Å—Ç—å")

        if features['pauses']['silence_ratio'] > 0.3:
            summary['speech_characteristics'].append("–º–Ω–æ–≥–æ –ø–∞—É–∑")
        else:
            summary['speech_characteristics'].append("–Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–∞—è —Ä–µ—á—å")

        return summary


def main():
    # 2025-10-09_08-52-53.022174_from_79851005767_to_79258972401_session_5396115979_talk_16k.wav  - –ø–ª–æ—Ö–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∑–≤—É–∫–∞. –ö–æ–≤–µ—Ä –∑–∞–±—Ä–∞–ª–∏ –Ω–µ —Å —Ç–æ–≥–æ —é—Ä –ª–∏—Ü–∞, —Å—Ä–æ—á–Ω–æ –ø–µ—Ä–µ–∑–≤–æ–Ω–∏—Ç—å
    audio_dir = 'audio_pool/'
    out_dir = 'transcriptions/'
    with open('quality_assessment.json', 'r') as f:
        quality_data = json.load(f)

    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ñ–∞–π–ª—ã —Å –∫–∞—á–µ—Å—Ç–≤–æ–º >= 7
    high_quality_files = [
        filename for filename in os.listdir(audio_dir)
        if filename.endswith('.wav')
           and filename in quality_data
           and quality_data[filename].get('quality_score', 0) >= 7
    ]

    print(f"–ù–∞–π–¥–µ–Ω–æ {len(high_quality_files)} —Ñ–∞–π–ª–æ–≤ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞")
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    analyzer = AudioAnalyzer(model_size="large")  # –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ "tiny" –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

    for filename in high_quality_files:

        audio_file = os.path.join(audio_dir, filename)
        output_filename = filename[:-4] + '.json'
        output_file = os.path.join(out_dir, output_filename)

        n = 0
        if output_filename in os.listdir(out_dir):
            n += 1
            continue
    print(f"{n} files already processed")

    for filename in high_quality_files:

        audio_file = os.path.join(audio_dir, filename)
        output_filename = filename[:-4] + '.json'
        output_file = os.path.join(out_dir, output_filename)

        n = 0
        if output_filename in os.listdir(out_dir):
            n += 1
            continue
        print(f"{n} files already processed")

        get_audio_info(audio_file)


        try:
            result = analyzer.analyze_audio_file(audio_file, output_file)

            # –í—ã–≤–æ–¥ –∫—Ä–∞—Ç–∫–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∫–æ–Ω—Å–æ–ª—å
            print("\n" + "=" * 50)
            print("–ö–†–ê–¢–ö–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
            print("=" * 50)
            print(f"–¢–µ–∫—Å—Ç: {result['transcription']['text'][:200]}...")
            print(f"–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {result['summary']['audio_duration_seconds']} —Å–µ–∫.")
            print(f"–Ø–∑—ã–∫: {result['transcription']['language']}")
            print(f"–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏: {', '.join(result['summary']['speech_characteristics'])}")
            print(f"–°—Ä–µ–¥–Ω—è—è –≤—ã—Å–æ—Ç–∞ —Ç–æ–Ω–∞: {result['audio_features']['pitch']['mean']:.2f} Hz")
            print(f"–í–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –≥—Ä–æ–º–∫–æ—Å—Ç–∏: {result['audio_features']['loudness']['std']:.4f}")

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ñ–∞–π–ª–∞: {e}")


if __name__ == "__main__":
    main()